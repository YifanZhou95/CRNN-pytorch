{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/datasets/home/79/279/cs253wec/warp-ctc/lib')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.optim.lr_scheduler as lrs\n",
    "\n",
    "import torchvision.utils as utils\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "from IIIT5K import IIIT5K\n",
    "from warpctc_pytorch import CTCLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=['-']+[chr(ord('a')+i) for i in range(26)]+[chr(ord('A')+i) for i in range(26)]+[chr(ord('0')+i) for i in range(10)]\n",
    "chrToindex={}\n",
    "indexTochr={}\n",
    "cnt=0\n",
    "for c in vocab:\n",
    "    chrToindex[c]=cnt\n",
    "    indexTochr[cnt]=c\n",
    "    cnt+=1\n",
    "vocab_size=cnt # uppercase and lowercase English characters and digits(26+26+10=62)\n",
    "batch_size=16\n",
    "sequence_len=28\n",
    "RNN_input_dim=7168\n",
    "RNN_hidden_dim=256\n",
    "RNN_layer=2\n",
    "RNN_type='LSTM'\n",
    "RNN_dropout=0\n",
    "use_VGG_extractor=False\n",
    "learning_rate=(4e-3)*(0.8**0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train=2000\n",
    "train_indices=range(num_train)\n",
    "num_test=3000\n",
    "\n",
    "mytransform = T.Compose(\n",
    "    [\n",
    "        T.Scale((224,224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]\n",
    ")\n",
    "IIIT5K_train = IIIT5K(\"IIIT5K-Word_V3.0\", mytransform, train=True)\n",
    "loader_train = DataLoader(dataset = IIIT5K_train,batch_size = batch_size, num_workers=2, \n",
    "                          sampler=SubsetRandomSampler(train_indices))\n",
    "\n",
    "IIIT5K_test = IIIT5K(\"IIIT5K-Word_V3.0\", mytransform, train=False)\n",
    "loader_test = DataLoader(dataset = IIIT5K_test,batch_size = 1, num_workers=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(loader_train)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print(images)\n",
    "# print(labels)\n",
    "\n",
    "imshow(utils.make_grid(images))\n",
    "print(' '.join('%s' % labels[j] for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CUDA available :',torch.cuda.is_available())\n",
    "cpu_dtype = torch.FloatTensor # the CPU datatype\n",
    "gpu_dtype = torch.cuda.FloatTensor # the GPU datatype\n",
    "\n",
    "dtype=gpu_dtype\n",
    "print(dtype)\n",
    "\n",
    "# From torchvision/vgg.py\n",
    "def reset(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.xavier_normal(m.weight, gain=1)\n",
    "#             n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "#             m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal(m.weight, gain=1)\n",
    "#             m.weight.data.normal_(0, 0.01)\n",
    "            m.bias.data.zero_()\n",
    "        elif hasattr(m, 'reset_parameters'):\n",
    "            m.reset_parameters()\n",
    "            \n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size() # read in N, C, H, W\n",
    "        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_block(nn.Module):\n",
    "    def __init__(self,in_channel,out_channel):\n",
    "        super(CNN_block, self).__init__()\n",
    "        self.conv_1=nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_2=nn.Conv2d(out_channel, out_channel, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_3=nn.Conv2d(out_channel, out_channel, kernel_size=3, stride=1, padding=1)\n",
    "        self.batchnorm1=nn.BatchNorm2d(num_features=out_channel)\n",
    "        self.batchnorm2=nn.BatchNorm2d(num_features=out_channel)\n",
    "        self.batchnorm3=nn.BatchNorm2d(num_features=out_channel)\n",
    "        self.relu=nn.ReLU(True)\n",
    "        self.maxpool=nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.relu(self.batchnorm1(self.conv_1(x)))\n",
    "        x=self.relu(self.batchnorm2(self.conv_2(x)))\n",
    "        x=self.relu(self.batchnorm3(self.conv_3(x)))\n",
    "        x=self.maxpool(x)\n",
    "        return x\n",
    "    \n",
    "class ToRNN(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x=x.permute(3,0,1,2)\n",
    "        W,N,C,H= x.size()\n",
    "        x.contiguous()\n",
    "        return x.view(W,N,-1)\n",
    "    \n",
    "class BiDireRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiDireRNN, self).__init__()\n",
    "        self.hidden_dim = RNN_hidden_dim\n",
    "        self.num_layers=RNN_layer\n",
    "        self.sql=sequence_len\n",
    "        self.bsize=batch_size\n",
    "        self.dropout=RNN_dropout\n",
    "        self.rnn_type=RNN_type\n",
    "        self.rnn = self.rnn_layer()\n",
    "        self.hidden=None\n",
    "        self.init_hidden(batch_size)\n",
    "        \n",
    "    def rnn_layer(self):\n",
    "        if self.rnn_type=='RNN':\n",
    "            return nn.RNN(RNN_input_dim, self.hidden_dim, self.num_layers, dropout=self.dropout, bidirectional=True)\n",
    "        elif self.rnn_type=='LSTM':\n",
    "            return nn.LSTM(RNN_input_dim, self.hidden_dim, self.num_layers, dropout=self.dropout, bidirectional=True)\n",
    "        elif self.rnn_type=='GRU':\n",
    "            return nn.GRU(RNN_input_dim, self.hidden_dim, self.num_layers, dropout=self.dropout, bidirectional=True)\n",
    "        else:\n",
    "            raise AssertionError('unknown RNN type:',self.rnn_type)\n",
    "    \n",
    "    def init_hidden(self,bsize):\n",
    "        if self.rnn_type=='LSTM':\n",
    "            self.hidden=(Variable(torch.zeros(self.num_layers*2, bsize, self.hidden_dim).type(dtype)),\n",
    "                    Variable(torch.zeros(self.num_layers*2, bsize, self.hidden_dim).type(dtype)))\n",
    "        else:\n",
    "            self.hidden=Variable(torch.zeros(self.num_layers*2, bsize, self.hidden_dim).type(dtype))\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        rnn_out, self.hidden = self.rnn(x, self.hidden)\n",
    "        return rnn_out\n",
    "        \n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,use_VGG_extractor=False):\n",
    "        super(Model, self).__init__()\n",
    "        if use_VGG_extractor:\n",
    "            self.feature_extractor=nn.Sequential(*([vgg16.features[i] for i in range(17)]))\n",
    "            for param in self.feature_extractor.parameters():\n",
    "                param.requires_grad=False\n",
    "                \n",
    "        else:\n",
    "            self.feature_extractor=nn.Sequential(*([CNN_block(3,64),CNN_block(64,128),CNN_block(128,256)]))\n",
    "        self.toRNN=ToRNN()\n",
    "        self.RNN=BiDireRNN()\n",
    "        self.toTraget=nn.Linear(RNN_hidden_dim*2, vocab_size)\n",
    "        self.softmax=nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature=self.feature_extractor(x)\n",
    "        RNN_input=self.toRNN(feature)\n",
    "        RNN_out=self.RNN(RNN_input)\n",
    "        tag_scores = self.toTraget(RNN_out)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_model=Model(use_VGG_extractor=use_VGG_extractor).type(dtype)\n",
    "print(my_model)\n",
    "for param in my_model.named_parameters():\n",
    "    print(param[0],type(param[1].data),param[1].size(),param[1].requires_grad)\n",
    "\n",
    "loss_function = CTCLoss().type(dtype)\n",
    "# opt_parameters=my_model.parameters() if use_VGG_extractor==False else [my_model.RNN.parameters(),my_model.toTraget.parameters()]\n",
    "# print(opt_parameters)\n",
    "# optimizer = optim.Adam(opt_parameters, lr=4e-3)\n",
    "\n",
    "if use_VGG_extractor:\n",
    "    opt_parameters=list(my_model.RNN.parameters())+list(my_model.toTraget.parameters())\n",
    "    optimizer = optim.Adam(iter(opt_parameters), lr=learning_rate)\n",
    "else:\n",
    "    optimizer = optim.Adam(my_model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = lrs.StepLR(optimizer, step_size=20, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(max_epoch,print_every):\n",
    "\n",
    "    iter_each_epoch=num_train//batch_size\n",
    "    loss_his_train=[]\n",
    "\n",
    "    for epoch in range(max_epoch):\n",
    "        scheduler.step()\n",
    "        my_model.train()\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),\n",
    "              'start epoch %d/%d:' % (epoch+1,max_epoch),'learning_rate =',scheduler.get_lr()[0],\n",
    "              'sequence_len =',my_model.RNN.sql)\n",
    "        tot_loss=0\n",
    "        \n",
    "        it=0\n",
    "        for images,labels in loader_train:\n",
    "\n",
    "            X_var=Variable(images.type(dtype))\n",
    "            \n",
    "            out_size=Variable(torch.IntTensor([sequence_len] * batch_size))\n",
    "            y_size=Variable(torch.IntTensor([len(l) for l in labels]))\n",
    "            conc_label=''.join(labels)\n",
    "            y=[chrToindex[c] for c in conc_label]\n",
    "            y_var=Variable(torch.IntTensor(y))\n",
    "\n",
    "            my_model.zero_grad()\n",
    "\n",
    "            my_model.RNN.init_hidden(batch_size)\n",
    "\n",
    "            scores = my_model(X_var)\n",
    "\n",
    "            loss = loss_function(scores,y_var,out_size,y_size)/batch_size\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            tot_loss+=loss.data[0]\n",
    "            \n",
    "            if it==0 or (it+1)%print_every==0 or it==iter_each_epoch-1:\n",
    "                print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),\n",
    "                      'iter %d loss = %f' % (it+1,loss.data[0]))\n",
    "            it+=1\n",
    "                \n",
    "        tot_loss/=iter_each_epoch\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),\n",
    "                'epoch %d/%d average_loss = %f\\n' % (epoch+1,max_epoch,tot_loss))\n",
    "        loss_his_train.append(tot_loss)\n",
    "    return loss_his_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_model.apply(reset)\n",
    "# my_model.load_state_dict(torch.load('parameters-5000'))\n",
    "my_model.train()\n",
    "my_model.RNN.init_hidden(batch_size)\n",
    "loss_his_train=model_train(max_epoch=500,print_every=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# save parameters\n",
    "torch.save(my_model.state_dict(), 'parameters-5000')\n",
    "\n",
    "# load parameters\n",
    "# my_model.load_state_dict(torch.load('parameters'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "ptr,=plt.plot(range(len(loss_his_train)),loss_his_train,'r-')\n",
    "plt.xlabel('training epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('loss function on train set')\n",
    "# plt.legend((ptr),('train'))\n",
    "plt.savefig('model-l.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataiter = iter(loader_test)\n",
    "image_test, label_test = dataiter.next()\n",
    "\n",
    "imshow(utils.make_grid(image_test))\n",
    "print(label_test[0])\n",
    "\n",
    "x_var_test = Variable(image_test.type(dtype))\n",
    "out=my_model.feature_extractor(x_var_test)\n",
    "feature_map=out.cpu().data.numpy()\n",
    "print(feature_map.shape)\n",
    "sample_index=np.random.choice(256,4)\n",
    "plt.figure(figsize = (2,2))\n",
    "gs1 = gridspec.GridSpec(2, 2)\n",
    "gs1.update(wspace=0.025, hspace=0.05)\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(gs1[i])\n",
    "    feature_submap=feature_map[0,sample_index[i],:,:]\n",
    "    plt.imshow(feature_submap)\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "my_model.RNN.init_hidden(1)\n",
    "result=my_model(x_var_test)\n",
    "result_np=result.cpu().data.numpy()\n",
    "result_word=''\n",
    "for i in range(sequence_len):\n",
    "    ch=np.argmax(result_np[i,0,:])\n",
    "    result_word+=indexTochr[ch]\n",
    "print(result_word)\n",
    "\n",
    "out_size=Variable(torch.IntTensor([sequence_len]))\n",
    "y_size=Variable(torch.IntTensor([len(l) for l in label_test]))\n",
    "conc_label=''.join(label_test)\n",
    "y=[chrToindex[c] for c in conc_label]\n",
    "y_var=Variable(torch.IntTensor(y))\n",
    "loss=loss_function(result,y_var,out_size,y_size)\n",
    "print(result)\n",
    "print(y_var)\n",
    "print(loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataiter = iter(loader_train)\n",
    "image_train, label_train = dataiter.next()\n",
    "image_train=image_train[0,...].resize_([1]+list(image_train.size())[1:])\n",
    "\n",
    "imshow(utils.make_grid(image_train))\n",
    "print(label_train[0])\n",
    "\n",
    "x_var_train = Variable(image_train.type(dtype))\n",
    "out=my_model.feature_extractor(x_var_train)\n",
    "feature_map=out.cpu().data.numpy()\n",
    "print(feature_map.shape)\n",
    "sample_index=np.random.choice(256,4)\n",
    "plt.figure(figsize = (2,2))\n",
    "gs1 = gridspec.GridSpec(2, 2)\n",
    "gs1.update(wspace=0.025, hspace=0.05)\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(gs1[i])\n",
    "    feature_submap=feature_map[0,sample_index[i],:,:]\n",
    "    plt.imshow(feature_submap)\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "my_model.RNN.init_hidden(1)\n",
    "result=my_model(x_var_train)\n",
    "softmax=nn.Softmax(dim=2)\n",
    "result_sf=softmax(result)\n",
    "result_np=result.cpu().data.numpy()\n",
    "result_word=''\n",
    "for i in range(sequence_len):\n",
    "    ch=np.argmax(result_np[i,0,:])\n",
    "    result_word+=indexTochr[ch]\n",
    "print(result_word)\n",
    "\n",
    "out_size=Variable(torch.IntTensor([sequence_len]))\n",
    "y_size=Variable(torch.IntTensor([len(l) for l in label_train[0]]))\n",
    "conc_label=''.join(label_train[0])\n",
    "y=[chrToindex[c] for c in conc_label]\n",
    "y_var=Variable(torch.IntTensor(y))\n",
    "loss=loss_function(result,y_var,out_size,y_size)\n",
    "print(result_sf)\n",
    "print(y_var)\n",
    "print(loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
